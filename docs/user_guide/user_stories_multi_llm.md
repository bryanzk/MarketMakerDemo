# 多 LLM 策略评估用户故事

## 概述

本文档描述了多 LLM 策略评估系统的用户故事，用于指导开发和测试。

---

## US-ML-001: 交易员获取多模型策略建议

### 故事描述

**作为** 一名加密货币交易员  
**我希望** 同时获取 Gemini、OpenAI、Claude 三个模型的策略建议  
**以便于** 我可以比较不同 AI 的观点，做出更明智的交易决策

### 验收标准

```gherkin
场景: 获取三个模型的策略建议
  假设 我已配置好三个 LLM 的 API Key
  并且 当前市场数据已准备就绪
  当 我调用 MultiLLMEvaluator.evaluate(context)
  那么 我应该收到 3 个评估结果
  并且 每个结果包含策略类型、参数建议、置信度
  并且 每个结果包含模型名称标识

场景: 部分模型 API 失败时仍返回可用结果
  假设 我已配置好 Gemini 和 OpenAI 的 API Key
  并且 Claude API Key 未配置或无效
  当 我调用 MultiLLMEvaluator.evaluate(context)
  那么 我应该收到至少 2 个成功的评估结果
  并且 失败的模型结果标记为 parse_success=False

场景: 解析 LLM 返回的 JSON 响应
  假设 LLM 返回了包含策略建议的 JSON
  当 评估器解析响应时
  那么 应该正确提取 spread、skew_factor、confidence 等字段
  并且 如果 JSON 格式错误，应该使用默认值并标记解析失败
```

### 技术备注

- 使用 `concurrent.futures.ThreadPoolExecutor` 并行调用
- 每个模型调用独立，互不影响
- 超时设置：30 秒

---

## US-ML-002: 交易员比较模型建议的模拟表现

### 故事描述

**作为** 一名加密货币交易员  
**我希望** 对每个模型的策略建议运行模拟交易  
**以便于** 我可以量化比较哪个模型的建议表现最好

### 验收标准

```gherkin
场景: 为每个建议运行模拟交易
  假设 我已获取 3 个模型的策略建议
  当 评估器运行模拟时
  那么 每个建议都应该有对应的模拟结果
  并且 模拟结果包含 PnL、胜率、夏普比率
  并且 模拟使用相同的市场条件（价格、波动率、资金费率）

场景: 模拟使用建议的参数
  假设 Gemini 建议 spread=0.012, skew_factor=120
  当 为 Gemini 建议运行模拟时
  那么 模拟器应该使用 spread=0.012, skew_factor=120
  并且 不应该使用其他模型的参数

场景: 可配置模拟步数
  假设 我设置 simulation_steps=1000
  当 运行评估时
  那么 每个模拟应该运行 1000 步
  并且 结果中 simulation_steps 字段为 1000
```

### 技术备注

- 模拟器需要支持动态参数设置
- 每个模型的模拟独立运行，使用相同的随机种子保证公平
- 模拟结果记录在 `SimulationResult` 对象中

---

## US-ML-003: 交易员获取排名结果

### 故事描述

**作为** 一名加密货币交易员  
**我希望** 看到按综合评分排名的结果  
**以便于** 我可以快速识别最佳策略建议

### 验收标准

```gherkin
场景: 按综合评分排名
  假设 三个模型都返回了有效的评估结果
  当 计算排名时
  那么 结果应该按 score 降序排列
  并且 第一名的 rank=1，第二名的 rank=2，依此类推
  并且 评分公式为: PnL*0.4 + Sharpe*0.3 + WinRate*0.2 + Confidence*0.1

场景: 获取最佳建议
  假设 评估结果已排名
  当 调用 get_best_proposal(results)
  那么 应该返回 rank=1 的结果
  并且 该结果的 score 是所有结果中最高的

场景: 解析失败的结果排名靠后
  假设 有一个模型的响应解析失败
  当 计算排名时
  那么 该模型的 score 应该为 0
  并且 该模型的 rank 应该是最后一名
```

### 技术备注

- 评分归一化到 0-100 范围
- 解析失败的结果得分为 0
- 使用稳定排序保证一致性

---

## US-ML-004: 交易员查看对比表格

### 故事描述

**作为** 一名加密货币交易员  
**我希望** 看到格式化的对比表格  
**以便于** 我可以一目了然地比较各模型的建议和结果

### 验收标准

```gherkin
场景: 生成对比表格
  假设 有 3 个评估结果
  当 调用 generate_comparison_table(results)
  那么 应该返回格式化的字符串表格
  并且 表格包含排名、模型名、策略、参数、PnL、胜率、夏普、评分
  并且 表格按排名顺序显示

场景: 表格显示关键指标
  假设 Claude 的 PnL=$200, 胜率=60%, 夏普=2.5
  当 生成表格时
  那么 Claude 行应该显示 "$200.00"
  并且 显示 "60.0%"
  并且 显示 "2.50"

场景: 表格显示延迟信息
  假设 Gemini 响应延迟 1250ms
  当 生成表格时
  那么 Gemini 行的 Latency 列应该显示 "1250ms"
```

### 技术备注

- 使用固定宽度格式保证对齐
- 数值格式化：PnL 带货币符号，百分比带 %，小数保留 2 位
- 支持导出为 JSON 格式

---

## US-ML-005: 交易员查看建议理由

### 故事描述

**作为** 一名加密货币交易员  
**我希望** 了解每个模型给出建议的理由  
**以便于** 我可以理解 AI 的思考过程并评估其合理性

### 验收标准

```gherkin
场景: 建议包含理由说明
  假设 LLM 返回了 reasoning 字段
  当 解析响应时
  那么 StrategyProposal.reasoning 应该包含理由文本
  并且 理由应该解释为什么选择该策略和参数

场景: 建议包含风险等级
  假设 LLM 返回了 risk_level 字段
  当 解析响应时
  那么 StrategyProposal.risk_level 应该是 "low"、"medium" 或 "high"

场景: 建议包含预期收益
  假设 LLM 返回了 expected_return 字段
  当 解析响应时
  那么 StrategyProposal.expected_return 应该是浮点数
```

### 技术备注

- reasoning 字段可以是中文或英文
- 前端需要支持展开/折叠长文本
- 考虑添加情感分析判断理由的乐观/悲观程度

---

## US-ML-006: 系统测量 LLM 响应延迟

### 故事描述

**作为** 系统管理员  
**我希望** 知道每个 LLM 的响应时间  
**以便于** 我可以监控 API 性能并优化体验

### 验收标准

```gherkin
场景: 记录每个模型的响应延迟
  当 调用 LLM API 时
  那么 应该记录从发送请求到收到响应的时间
  并且 时间以毫秒为单位存储在 latency_ms 字段

场景: 延迟包含在结果中
  假设 OpenAI 响应用时 980ms
  当 评估完成时
  那么 OpenAI 的 EvaluationResult.latency_ms 应该约等于 980

场景: 超时处理
  假设 某个模型响应超过 30 秒
  当 等待响应时
  那么 应该返回超时错误
  并且 该模型的结果标记为失败
```

### 技术备注

- 使用 `time.time()` 或 `time.perf_counter()` 测量
- 超时使用 `concurrent.futures` 的 timeout 参数
- 日志记录延迟便于后续分析

---

## US-ML-007: 交易员应用最佳策略

### 故事描述

**作为** 一名加密货币交易员  
**我希望** 一键应用排名第一的策略建议  
**以便于** 我可以快速将 AI 建议转化为实际交易参数

### 验收标准

```gherkin
场景: 应用最佳策略参数
  假设 Claude 是排名第一的建议
  并且 Claude 建议 spread=0.010, skew_factor=150
  当 我调用应用策略功能
  那么 当前策略的 spread 应该更新为 0.010
  并且 skew_factor 应该更新为 150

场景: 应用前经过风控验证
  假设 最佳建议的 spread=0.001（过小）
  当 尝试应用策略时
  那么 RiskAgent 应该拒绝该参数
  并且 返回拒绝原因
  并且 当前策略参数不变

场景: 记录策略变更日志
  假设 策略参数被成功更新
  当 应用完成时
  那么 应该记录变更日志
  并且 日志包含变更前后的参数值
  并且 日志包含建议来源（哪个模型）
```

### 技术备注

- 与现有的 QuantAgent 和 RiskAgent 集成
- 支持回滚到之前的参数
- 可选的人工确认步骤

---

## US-ML-008: 交易员查看历史评估记录

### 故事描述

**作为** 一名加密货币交易员  
**我希望** 查看历史评估记录  
**以便于** 我可以分析哪个模型长期表现更好

### 验收标准

```gherkin
场景: 保存评估记录
  假设 我完成了一次多模型评估
  当 评估结束时
  那么 结果应该被保存到历史记录
  并且 记录包含时间戳、市场状态、所有模型的建议和结果

场景: 查询历史记录
  假设 我想查看过去 7 天的评估
  当 我调用历史查询 API
  那么 应该返回过去 7 天的所有评估记录
  并且 按时间倒序排列

场景: 统计模型表现
  假设 我有 30 天的评估历史
  当 我请求模型表现统计
  那么 应该返回每个模型的平均排名
  并且 返回每个模型的胜出次数（排名第一的次数）
```

### 技术备注

- 使用 SQLite 或 JSON 文件存储历史
- 支持按时间范围、币种筛选
- 提供可视化图表（后续功能）

---

## 优先级矩阵

| 用户故事 | 优先级 | 复杂度 | MVP |
|---------|-------|-------|-----|
| US-ML-001 | P0 | 中 | ✅ |
| US-ML-002 | P0 | 中 | ✅ |
| US-ML-003 | P0 | 低 | ✅ |
| US-ML-004 | P0 | 低 | ✅ |
| US-ML-005 | P1 | 低 | ✅ |
| US-ML-006 | P1 | 低 | ✅ |
| US-ML-007 | P1 | 中 | ❌ |
| US-ML-008 | P2 | 高 | ❌ |

---

## 下一步

这些用户故事将指导测试用例的编写，详见 [测试用例文档](../../tests/test_multi_llm_evaluation.py)。

